{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from utils import *\n",
    "import copy\n",
    "import pprint\n",
    "import glob\n",
    "from data_load import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_withGNP = True\n",
    "ignoreLongDocument= False\n",
    "os.environ['BC6PM_dir'] = '../BC6PM'\n",
    "os.environ['pretrain_dir'] = '../BioBERT-Models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if train_withGNP:\n",
    "    train_data_dir = os.path.join(os.environ.get('BC6PM_dir'), 'GNormPlus', 'withAnn-Result', 'PMtask_Relations_TrainingSet_r.json')\n",
    "else:\n",
    "    train_data_dir = os.path.join(os.environ.get('BC6PM_dir'), 'json', 'PMtask_Relations_TrainingSet.json')\n",
    "with open(train_data_dir) as f:\n",
    "    train_data = json.load(f)\n",
    "documents_train = train_data['documents']\n",
    "with open(os.path.join(os.environ.get('BC6PM_dir'), 'GNormPlus', 'result', 'PMtask_Relations_TestSet_r.json')) as f:\n",
    "    eval_data = json.load(f)\n",
    "documents_eval = eval_data['documents']\n",
    "with open(os.path.join(os.environ.get('BC6PM_dir'), 'json', 'PMtask_Relations_TestSet.json')) as f:\n",
    "    eval_data_ground_truth = json.load(f)\n",
    "documents_eval_ground_truth = eval_data_ground_truth['documents']\n",
    "if ignoreLongDocument: # Ignore too long text\n",
    "    with open('pmid2tokenlen.json') as f:\n",
    "        pmid2tokenlen = json.load(f)\n",
    "    def filterLongDocu(documents):\n",
    "        documents_ = []\n",
    "        for document in documents:\n",
    "            if pmid2tokenlen.get(document['id'], 1) < 512: # some documents may not have RC instance\n",
    "                documents_.append(document)\n",
    "        return documents_\n",
    "    documents_train = filterLongDocu(documents_train)\n",
    "    documents_eval = filterLongDocu(documents_eval)\n",
    "    documents_eval_ground_truth = filterLongDocu(documents_eval_ground_truth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground Truth Relation Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation_num = []\n",
    "for document in documents_eval_ground_truth:\n",
    "    relation_num.append(len(document['relations']))\n",
    "relation_num = np.array(relation_num)\n",
    "print(relation_num.mean(), [(relation_num==i).sum() for i in range(0,10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eval Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relationFilter(documents, confidence_threshold = 0.5, at_least = 0, at_most = 4):\n",
    "    documents_rtn = copy.deepcopy(documents)\n",
    "    for i, document in enumerate(documents):\n",
    "        documents_rtn[i]['relations'] = []\n",
    "        relations =  sorted(document['relations'], key=lambda relation: relation['infons']['confidence'], reverse = True)\n",
    "        for j, relation in enumerate(relations):\n",
    "            if relation['infons']['confidence']  > th_conf:\n",
    "                 documents_rtn[i]['relations'].append(relation)\n",
    "        if len(documents_rtn[i]['relations']) < at_least and len(relations)>0:\n",
    "            documents_rtn[i]['relations'] = [relations[:at_least]]\n",
    "        if len(documents_rtn[i]['relations']) > at_most and at_most > 0:\n",
    "            documents_rtn[i]['relations'] = relations[:at_most]\n",
    "    return documents_rtn\n",
    "def countDocumentNums(documents_pred_all_relation, gold_standard_relations, confidence_threshold = 0.5):\n",
    "    \"\"\"\n",
    "    if `documents` is `documents_filtered` :\n",
    "    return (pred_positive_num, true_positive_num, false_positive_num, relation2PredResult) \n",
    "    if `documents` is `documents_all_relations`:\n",
    "    return (pred_positive_num, label_1_num, label_0_num, relation2GroundTruthLabel)\n",
    "    \"\"\"\n",
    "    pred_pos = 0\n",
    "    label_1_num_or_tp_num = 0\n",
    "    label_0_num_or_fp_num = 0\n",
    "    relation2Result = {}\n",
    "    for document in documents_pred_all_relation:\n",
    "        for relation in document['relations']:\n",
    "            if  relation['infons']['confidence'] >= confidence_threshold:\n",
    "                pred_pos += 1\n",
    "\n",
    "            geneids = [relation['infons']['Gene1'], relation['infons']['Gene2']]\n",
    "            geneids.sort()\n",
    "            relation_str = 'PMID' + document['id'] + '_' + '_'.join(geneids)\n",
    "            if relation_str in gold_standard_relations:\n",
    "                label_1_num_or_tp_num += 1\n",
    "                relation2Result[relation_str] = 1\n",
    "            else:\n",
    "                label_0_num_or_fp_num += 1\n",
    "                relation2Result[relation_str] = 0\n",
    "        \n",
    "    return pred_pos, label_1_num_or_tp_num, label_0_num_or_fp_num, relation2Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Post process of outputjson data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gold_standard_ids, gold_standard_relations = JSON_Collection_Relation({'documents': documents_eval_ground_truth})\n",
    "th_conf = 0.5 # threshold of confidence\n",
    "at_least = 0  # assume the minimal/maximal number of PPIm of one single document in predicited relations at least/most  \n",
    "at_most = 4\n",
    "fileList = glob.glob('outputjson/*Test*.json')\n",
    "fileList = sorted(fileList, key = lambda f : (f.split(\".json\")[0].split(\"_\")[0], int(f.split(\".json\")[0].split(\"_\")[1])))\n",
    "print('\\t'.join(['Setting'+' '*42, 'Epoch', 'R-GNP' ,'P-Ext', 'R-Ext', 'F1-Ext', 'P-Homo', 'R-Homo', 'F1-Homo']))\n",
    "for fileName in fileList:\n",
    "    name, idx = fileName.split(\".json\")[0].split(\"_\")\n",
    "    if int(idx) < 11:\n",
    "        print(\"{:50}\".format(name.split(\"/\")[1]), idx,sep = '\\t', end = '\\t')\n",
    "    else:\n",
    "        continue    \n",
    "    with open(fileName) as f:\n",
    "        documents_all_relations = json.load(f)['documents']\n",
    "    documents_filtered = relationFilter(documents_all_relations, confidence_threshold= th_conf, at_least = at_least, at_most = at_most)\n",
    "\n",
    "    pred_positive_num, label_1_num, label_0_num, r2truth = countDocumentNums(documents_all_relations, gold_standard_relations)\n",
    "    pred_positive_num, true_positive_num, false_positive_num, r2pred = countDocumentNums(documents_filtered, gold_standard_relations)\n",
    "\n",
    "    # R-GNP: recall of all gene pair generated by GNP, which means ignoring recognized genes.\n",
    "    print(\"{:.2f}\".format(100*true_positive_num/label_1_num), end='\\t')\n",
    "\n",
    "    # Exact match\n",
    "    micro_precision, micro_recall, micro_f1, macro_precision, macro_recall, macro_f1 = Classification_Performance_Relation({'documents': documents_filtered}, gold_standard_ids, gold_standard_relations, homolo = False)\n",
    "    print(\"{:.2f}\\t{:.2f}\\t{:.2f}\".format(100*micro_precision, 100*micro_recall, 100*micro_f1), end = '\\t')\n",
    "    # Homolo match\n",
    "    micro_precision, micro_recall, micro_f1, macro_precision, macro_recall, macro_f1 = Classification_Performance_Relation({'documents': documents_filtered}, gold_standard_ids, gold_standard_relations, homolo = True)\n",
    "    print(\"{:.2f}\\t{:.2f}\\t{:.2f}\".format(100*micro_precision, 100*micro_recall, 100*micro_f1))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### collect result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileList = glob.glob('outputjson/*Test*.json')\n",
    "fileList = sorted(fileList, key = lambda f : (f.split(\".json\")[0].split(\"_\")[0], int(f.split(\".json\")[0].split(\"_\")[1])))\n",
    "targetFileName = {\n",
    "               'rc_triage': 'Joint-RC-triage-Test-fineTune-GNP-weightLabel_10',\n",
    "                'rc':  'RC-Only-Test-fineTune-GNP-weightLabel_10',\n",
    "                'rc_ner':  'Joint-RC-NER-SaveModel-Test-fineTune-GNP_10'\n",
    "}\n",
    "settings = sorted(targetFileName.keys())\n",
    "r2pred_dict = {}\n",
    "setting2documents_all_relations = {}\n",
    "for setting in settings:\n",
    "    path = os.path.join('outputjson', targetFileName[setting]+'.json')\n",
    "    print(setting.upper())\n",
    "    with open(path) as f:\n",
    "        setting2documents_all_relations[setting] = json.load(f)['documents']\n",
    "    documents_filtered = relationFilter(setting2documents_all_relations[setting], confidence_threshold= th_conf, at_least = at_least, at_most = at_most)\n",
    "\n",
    "    pred_positive_num, label_1_num, label_0_num, r2truth = countDocumentNums(setting2documents_all_relations[setting], gold_standard_relations)\n",
    "    print('pred_positive', pred_positive_num,'label_1', label_1_num,'label_0', label_0_num, sep = '\\t')\n",
    "    pred_positive_num, true_positive_num, false_positive_num, r2pred_dict[setting] = countDocumentNums(documents_filtered, gold_standard_relations)\n",
    "    print('pred_positive', pred_positive_num, 'tp', true_positive_num,'fp',  false_positive_num, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_result2relation = {}\n",
    "print('\\t'.join(['pmid', 'g1', 'g2', \"label\"]+settings))\n",
    "for truth_label_target in [0, 1]:\n",
    "    notFound = 'false_neg' if truth_label_target == 1 else 'true_neg'\n",
    "    predResult ={1:'true', 0: 'false', 'true_neg': 'true', 'false_neg': 'false'}\n",
    "\n",
    "    for relation_str, label in r2truth.items():\n",
    "        if label == truth_label_target:\n",
    "            pred_result = [label] + [predResult[r2pred_dict[setting].get(relation_str, notFound)] for setting in settings]\n",
    "            pred_result = tuple(pred_result)\n",
    "            print('\\t'.join(relation_str[4:].split('_')), '\\t'.join([str(x) for x in pred_result]), sep ='\\t')\n",
    "            if pred_result2relation.get(pred_result) is None:\n",
    "                    pred_result2relation[pred_result] = []\n",
    "            pred_result2relation[pred_result].append(relation_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t'.join(['label'] +settings+ [ 'cnt']) )\n",
    "pred_result_cnt = { k: len(v) for k, v in pred_result2relation.items()}\n",
    "for k, v in pred_result_cnt.items():\n",
    "    print( '\\t'.join([str(x) for x in k]), v, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDocumentByRelation(documents, relation_str):\n",
    "    pmid, g1,g2 = relation_str[4:].split('_')\n",
    "    for document in documents:\n",
    "        if document['id'] != pmid:\n",
    "            continue\n",
    "        for relation in document['relations']:\n",
    "            geneids = [relation['infons']['Gene1'], relation['infons']['Gene2']]\n",
    "            geneids.sort()\n",
    "            if geneids == [g1, g2]:\n",
    "                return document, relation\n",
    "def printGeneMentionInRelation(document, relation_str):\n",
    "    pmid, g1,g2 = relation_str[4:].split('_')\n",
    "    geneid2text = {}\n",
    "    geneid2text[g1] = []\n",
    "    geneid2text[g2] = []\n",
    "    for ann in (document['passages'][0]['annotations'] + document['passages'][1]['annotations']):\n",
    "        if Annotation.gettype(ann) == 'Gene':\n",
    "            geneid = Annotation.getNCBIID(ann)\n",
    "            if geneid in [g1, g2]:\n",
    "                geneid2text[geneid].append(ann['text'])\n",
    "    for id_, texts in geneid2text.items():\n",
    "        if id_ == g1:\n",
    "            print(\"GeneA\", end='\\t')\n",
    "        else:\n",
    "            print(\"GeneB\", end = '\\t')\n",
    "        print(f\"ID: {id_}\", '; Mentions:' if len(set(texts)) > 1 else 'Mention:', ','.join(set(texts)))\n",
    "\n",
    "def pprintline(text, width = 50):\n",
    "    dashNum = width - len(text)\n",
    "    offset = dashNum //2\n",
    "    print('-'*(dashNum-offset), text, '-'*(dashNum+offset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# result = (0, 'false', 'true', 'true')\n",
    "result = (1, 'true', 'true', 'true')\n",
    "pprintline(\"Result\")\n",
    "print('label','\\t'.join(settings), sep = '\\t')\n",
    "print(result[0],'\\t'.join(result[1:]), sep='\\t')\n",
    "cnt = 0\n",
    "for relation_str in pred_result2relation[result]:\n",
    "    pprintline('PMID&GeneID')\n",
    "\n",
    "    setting2relation = {}\n",
    "    print( '\\t'.join(relation_str[4:].split('_')))\n",
    "    pprintline('Confidence')\n",
    "    for setting, documents_all_relations in setting2documents_all_relations.items():\n",
    "        document, relation = getDocumentByRelation( documents_all_relations, relation_str)\n",
    "        setting2relation[setting] = relation\n",
    "        print(f\"{setting:10}\", f\"{relation['infons']['confidence']:.2%}\", sep = '\\t')\n",
    "    pprintline('Gene Mention')\n",
    "    printGeneMentionInRelation(document, relation_str)\n",
    "    pprintline('Original Text')\n",
    "    print(document['passages'][0]['text'], document['passages'][1]['text'])\n",
    "    dataset = RCDataSet([document],os.environ.get('pretrain_dir') )\n",
    "    for example in dataset.examples:\n",
    "        if example.guid == relation_str[4:]:\n",
    "            pprintline('Pre-processed Text')\n",
    "            preprocessedText = example.text_a\n",
    "            for word in preprocessedText.split():\n",
    "                if word == \"Gene_A\":\n",
    "                    formated = f\"\\033[0;37;41m{word}\\033[0m\"\n",
    "                elif word == \"Gene_B\":\n",
    "                    formated = f\"\\033[0;37;40m{word}\\033[0m\"\n",
    "                elif word == \"Gene_S\":\n",
    "                    formated = f\"\\033[0;37;40m{word}\\033[0m\"\n",
    "                elif word == \"Gene_N\":\n",
    "                    formated = f\"\\033[0;30;43m{word}\\033[0m\"\n",
    "                else:\n",
    "                    formated = word\n",
    "                print(formated, end =' ')\n",
    "            print()\n",
    "#             print(preprocessedText)\n",
    "            ids = dataset.collate_fn([example])[0]['input_ids'][0]\n",
    "            pprintline('To BERT')\n",
    "            tokens = dataset.tokenizer.convert_ids_to_tokens(ids)\n",
    "            print(dataset.tokenizer.decode(ids))\n",
    "            pprintline('Pre-processed Text For Latex')\n",
    "            preprocessedText = example.text_a\n",
    "\n",
    "            for word in preprocessedText.split():\n",
    "                if word == \"Gene_A\":\n",
    "                    formated = \"\\colorbox{blue-wy}{Gene\\_A}\"\n",
    "                elif word == \"Gene_B\":\n",
    "                    formated = \"\\colorbox{mossgreen}{Gene\\_B}\"\n",
    "                elif word == \"Gene_S\":\n",
    "                    formated = \"\\colorbox{plum(web)}{Gene\\_S}\"\n",
    "                elif word == \"Gene_N\":\n",
    "                    formated = \"\\colorbox{peach}{Gene\\_N}\"\n",
    "                else:\n",
    "                    formated = word\n",
    "                print(formated, end =' ')\n",
    "            print()\n",
    "    cnt += 1\n",
    "    if cnt > 3:\n",
    "        break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Unfinished) Gene Distribution in Sentences 在句子中分布情况 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_document_process(document):\n",
    "    pmid = document['id']\n",
    "    relations = set()\n",
    "    genes = set()\n",
    "    text_li = []\n",
    "    passages = document['passages']\n",
    "    split_word = re.compile(r\"\\w+|\\S\")\n",
    "    for r in document['relations']:\n",
    "        relations.add((r['infons']['Gene1'], r['infons']['Gene2']))\n",
    "\n",
    "    for passage in passages:\n",
    "        anns = passage['annotations']\n",
    "        text = passage['text']\n",
    "        offset_p = passage['offset']\n",
    "        index = 0\n",
    "        if len(anns) == 0:\n",
    "            text_li.extend(split_word.findall(text))\n",
    "        else:\n",
    "            anns = Annotation.sortAnns(anns)\n",
    "            for ann in anns:\n",
    "                if Annotation.gettype(ann) == 'Gene':\n",
    "                    for infon_type in ann['infons']:\n",
    "                        if infon_type.lower() == 'ncbi gene':\n",
    "                            genes.add(ann['infons'][infon_type])\n",
    "                else:\n",
    "                    continue\n",
    "                for i, location in enumerate(ann['locations']):\n",
    "                    offset = location['offset']\n",
    "                    length = location['length']\n",
    "                    text_li.extend(split_word.findall(\n",
    "                        text[index:offset-offset_p]))\n",
    "                    if i == len(ann['locations']) - 1:\n",
    "                        ncbi_gene_id = Annotation.getNCBIID(ann)\n",
    "                        # for infon_type in ann['infons']:\n",
    "                            # if infon_type.lower() in ['ncbi gene', 'identifier']:\n",
    "                                # ncbi_gene_id = ann['infons'][infon_type]\n",
    "                        text_li.append(\"Gene_{}\".format(ncbi_gene_id))\n",
    "                    index = max(offset - offset_p + length, index)\n",
    "            text_li.extend(split_word.findall(text[index:]))\n",
    "    return pmid, text_li, genes, relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(os.environ.get('BC6PM_dir'), 'GNormPlus', 'withAnn-Result', 'PMtask_Relations_TrainingSet_r.json')) as f:\n",
    "    collection = json.load(f)\n",
    "    documents = collection['documents']\n",
    "pmid, text_li, genes, relations = train_document_process(documents[0])\n",
    "for sent in (' '.join(text_li)).split(\".\"):\n",
    "    sent_li = sent.split(' ')\n",
    "    print(sent_li)\n",
    "    for gene in genes:\n",
    "        if f\"Gene_{gene}\" in sent_li:\n",
    "            print(gene, 'exist')\n",
    "print(genes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
